{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDb reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGoal: obtaining the following data structure\\n\\n|===============|\\n|==> reviews <==|\\n|===============|\\n\\n[\\n    {\\n        \\'text\\': \"plain text of rvw\",  # string\\n        \\'is_positive\\': True,          # bool\\n        \\'tokens\\': [                   # list of strings\\n                    \\'t1\\',\\n                    \\'t1\\',\\n                    \\'t2\\',\\n                    ...\\n                  ]\\n        \\'tf\\':     {                   # dictionary | key:token | val:occurrence\\n                    \\'t1\\': 2,\\n                    \\'t2\\': 1,\\n                    ...\\n                  }\\n        \\'tf_idf\\': {                   # dictionary | key:token | val:weight\\n                    \\'t1\\': 3.2,\\n                    \\'t2\\': 6.2,\\n                    ...\\n                  }\\n    },\\n    ...\\n]\\n\\n|==============| All tokens\\n|==> tokens <==| of all documents\\n|==============| without duplicates\\n\\n[\\n    \\'token_1\\',\\n    \\'token_2\\',\\n    \\'token_3\\',\\n    ...\\n]\\n\\n|==========================| Number of document in which\\n|==> document_frequency <==| a given token\\n|==========================| appears at least once\\n\\n{\\n    \\'token_1\\': 2,\\n    \\'token_2\\': 1,\\n    \\'token_3\\': 19,\\n    ...\\n}\\n\\n|==================================| Helps to find significance\\n|==> inverse_document_frequency <==| of a token among the entire\\n|==================================| collection of documents\\n{\\n    \\'token_1\\': 10.15,\\n    \\'token_2\\': 11.32,\\n    \\'token_3\\': 4.68,\\n    ...\\n}\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import math\n",
    "import collections\n",
    "import random\n",
    "\n",
    "FILENAME = './datasets/IMDb_reviews.txt'\n",
    "NUMBER_OF_DOCUMENTS = -1\n",
    "\n",
    "\"\"\"\n",
    "Goal: obtaining the following data structure\n",
    "\n",
    "|===============|\n",
    "|==> reviews <==|\n",
    "|===============|\n",
    "\n",
    "[\n",
    "    {\n",
    "        'text': \"plain text of rvw\",  # string\n",
    "        'is_positive': True,          # bool\n",
    "        'tokens': [                   # list of strings\n",
    "                    't1',\n",
    "                    't1',\n",
    "                    't2',\n",
    "                    ...\n",
    "                  ]\n",
    "        'tf':     {                   # dictionary | key:token | val:occurrence\n",
    "                    't1': 2,\n",
    "                    't2': 1,\n",
    "                    ...\n",
    "                  }\n",
    "        'tf_idf': {                   # dictionary | key:token | val:weight\n",
    "                    't1': 3.2,\n",
    "                    't2': 6.2,\n",
    "                    ...\n",
    "                  }\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "\n",
    "|==============| All tokens\n",
    "|==> tokens <==| of all documents\n",
    "|==============| without duplicates\n",
    "\n",
    "[\n",
    "    'token_1',\n",
    "    'token_2',\n",
    "    'token_3',\n",
    "    ...\n",
    "]\n",
    "\n",
    "|==========================| Number of document in which\n",
    "|==> document_frequency <==| a given token\n",
    "|==========================| appears at least once\n",
    "\n",
    "{\n",
    "    'token_1': 2,\n",
    "    'token_2': 1,\n",
    "    'token_3': 19,\n",
    "    ...\n",
    "}\n",
    "\n",
    "|==================================| Helps to find significance\n",
    "|==> inverse_document_frequency <==| of a token among the entire\n",
    "|==================================| collection of documents\n",
    "{\n",
    "    'token_1': 10.15,\n",
    "    'token_2': 11.32,\n",
    "    'token_3': 4.68,\n",
    "    ...\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILENAME, encoding=\"utf8\") as f:\n",
    "    # Ignore the first line\n",
    "    f.readline()\n",
    "\n",
    "    # Initialize the reviews list\n",
    "    reviews = []\n",
    "    \n",
    "    # Each row in the file contains a review and an integer (1: positive, 0: negative)\n",
    "    for review in f:\n",
    "        try:\n",
    "            review_text_match = re.findall(r'(.+),(\\d)', review)\n",
    "            review_text = review_text_match[0][0].replace('\"\"', '\"') \n",
    "\n",
    "            reviews.append({\n",
    "                'text': review_text,\n",
    "                'is_positive': (review_text_match[0][1] == '1')\n",
    "            })\n",
    "        except:\n",
    "            print('>>ERROR with following review:')\n",
    "            print(review)\n",
    "\n",
    "\n",
    "NUMBER_OF_DOCUMENTS = len(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'tokens' list to each review\n",
    "\n",
    "def getTokensFromDocument(document):\n",
    "    \"\"\"\n",
    "    Compute the tokens for a given document\n",
    "    \n",
    "    Input: a string. It is a document to tokenize\n",
    "    Output: a list. It contains the tokens of the relative document\n",
    "    \"\"\"\n",
    "\n",
    "    for punct in string.punctuation:\n",
    "        document = document.replace(punct, \" \")\n",
    "    tokens = [ token.lower() for token in document.split(\" \") if token ]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "for review in reviews:\n",
    "    review['tokens'] = getTokensFromDocument(review['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Term Frequency (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'tf' dictionary to each review\n",
    "\n",
    "def getTF(tokens):\n",
    "    \"\"\"Get the Time Frequency for each token\"\"\"\n",
    "    tf = {}\n",
    "    for token in tokens:\n",
    "        if token in tf:\n",
    "            tf[token] += 1\n",
    "        else:\n",
    "            tf[token] = 1\n",
    "    return tf\n",
    "\n",
    "for review in reviews:\n",
    "    review['tf'] = getTF(review['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Document Frequency (DF) and Inverse DF (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nN: number of documents\\nDF(t): document frequency of a token (the number of documents in which t appears at least once)\\n\\nIDF(t) = log( N / DF(t) )\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "N: number of documents\n",
    "DF(t): document frequency of a token (the number of documents in which t appears at least once)\n",
    "\n",
    "IDF(t) = log( N / DF(t) )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a. Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDFDictionary(documents):\n",
    "    \"\"\"\n",
    "    Input: a list of documents. It is required to have 'tf' in the document\n",
    "    Output: a dictionary. It contains token as keys and DF(token) as value\n",
    "    \"\"\"\n",
    "    DF_dictionary = {}\n",
    "    \n",
    "    # Foreach document\n",
    "    for document in documents:\n",
    "        time_frequency = document['tf']\n",
    "        \n",
    "        # Foreach token in the time_frequency dictionary\n",
    "        for token in time_frequency:\n",
    "            \n",
    "            # If the current token was already added before\n",
    "            if token in DF_dictionary:\n",
    "                DF_dictionary[token] += 1\n",
    "                \n",
    "            else:\n",
    "                DF_dictionary[token] = 1\n",
    "                \n",
    "    return DF_dictionary\n",
    "\n",
    "# DF\n",
    "\"\"\"\n",
    "To get the DF of a token:\n",
    "    document_frequency['token_string']\n",
    "\"\"\"\n",
    "document_frequency = getDFDictionary(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.b. Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIDFDictionary(DF_dictionary, N):\n",
    "    \"\"\"\n",
    "    Input: a dictionary (DF), an integer (number of documents)\n",
    "    Output: a dictionary. It contains token as keys and IDF(token) as value\n",
    "    \"\"\"\n",
    "    IDF_dictionary = {}\n",
    "    \n",
    "    # Foreach [key:token, val:DF(token)] in the DF dictionary\n",
    "    for token, DF_t in DF_dictionary.items():\n",
    "        IDF_dictionary[token] = math.log(N / DF_t)\n",
    "    \n",
    "    return IDF_dictionary\n",
    "\n",
    "# IDF\n",
    "\"\"\"\n",
    "To get the IDF of a token:\n",
    "    inverse_document_frequency['token_string']\n",
    "\"\"\"\n",
    "inverse_document_frequency = getIDFDictionary(document_frequency, NUMBER_OF_DOCUMENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.c. Sorted IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 tokens with the lowest IDF (most common ones):\n",
      " > the\t\t- IDF: 0.008314469604085238\n",
      " > a\t\t- IDF: 0.03351541933781697\n",
      " > and\t\t- IDF: 0.03401190259170586\n",
      " > of\t\t- IDF: 0.05226218466281087\n",
      " > to\t\t- IDF: 0.06293979977387414\n",
      " > this\t\t- IDF: 0.09924591465797242\n",
      " > is\t\t- IDF: 0.1086102347240488\n",
      " > it\t\t- IDF: 0.11536595914077863\n",
      " > in\t\t- IDF: 0.12606221366364628\n",
      " > that\t\t- IDF: 0.20722099077039452\n",
      "10 tokens with the highest IDF (most uncommon ones):\n",
      " > capiche\t\t- IDF: 10.126631103850338\n",
      " > camora\t\t- IDF: 10.126631103850338\n",
      " > jowls\t\t- IDF: 10.126631103850338\n",
      " > repleat\t\t- IDF: 10.126631103850338\n",
      " > jayden\t\t- IDF: 10.126631103850338\n",
      " > imy\t\t- IDF: 10.126631103850338\n",
      " > orientalist\t\t- IDF: 10.126631103850338\n",
      " > rouÃ©\t\t- IDF: 10.126631103850338\n",
      " > infantalising\t\t- IDF: 10.126631103850338\n",
      " > ant1\t\t- IDF: 10.126631103850338\n"
     ]
    }
   ],
   "source": [
    "# 10 lowest IDF\n",
    "asc_sorted_IDF = sorted(inverse_document_frequency.items(), key=lambda kv: kv[1])\n",
    "\n",
    "print('10 tokens with the lowest IDF (most common ones):')\n",
    "for token in asc_sorted_IDF[:10]:\n",
    "    print(\" > \" + token[0] + \"\\t\\t- IDF: \" + str(token[1]))\n",
    "    \n",
    "# 10 highest IDF\n",
    "desc_10_sorted_IDF = asc_sorted_IDF[-10:]\n",
    "desc_10_sorted_IDF.reverse()\n",
    "\n",
    "print('10 tokens with the highest IDF (most uncommon ones):')\n",
    "for token in desc_10_sorted_IDF:\n",
    "    print(\" > \" + token[0] + \"\\t\\t- IDF: \" + str(token[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in reviews:\n",
    "    # Dictionary with tokens of the current document as key and TF-IDF for that token in this document as value\n",
    "    current_TF_IDF = {}\n",
    "    \n",
    "    for token in review['tokens']:\n",
    "        # Frequency of the current token in the current document\n",
    "        current_TF = review['tf'][token]\n",
    "        \n",
    "        # IDF of the current token\n",
    "        current_IDF = inverse_document_frequency[token]\n",
    "        \n",
    "        # Get the current TF-IDF(token, document) value\n",
    "        current_TF_IDF[token] = current_TF * current_IDF\n",
    "    \n",
    "    review['tf_idf'] = current_TF_IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(d):\n",
    "    \"\"\"Compute the L2-norm of a vector representation\"\"\"\n",
    "    return sum([tf_idf ** 2 for t, tf_idf in d.items() ])**.5\n",
    "\n",
    "def dot_product(d1, d2):\n",
    "    \"\"\"Compute the dot product between two vector representations\"\"\"\n",
    "    word_set = set(list(d1.keys()) + list(d2.keys()))\n",
    "    return sum([ (d1.get(d, 0.0) * d2.get(d, 0.0)) for d in word_set ])\n",
    "\n",
    "def cosine_similarity(d1, d2):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between documents d1 and d2.\n",
    "    \n",
    "    Input: two dictionaries representing the TF-IDF vectors for documents d1 and d2.\n",
    "    Output: the cosine similarity.\n",
    "    \"\"\"\n",
    "    return dot_product(d1, d2) / (norm(d1) * norm(d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive similarity: 0.014762028785759492\n",
      "Negative similarity: 0.013608276884642494\n",
      "\n",
      "\n",
      "The super iper advanced artificial intelligence powered by deep learning and machine learning, working on a blockchain and analyzing big data says that the review was positive\n",
      "\n",
      "\n",
      "The truth is that it was positive\n",
      "\n",
      "\n",
      "Yeah, the AI is extreamly smart!\n"
     ]
    }
   ],
   "source": [
    "def identifyLabel(test_document, include_similarity=False):\n",
    "    # Compute the avg similarity by dividing the sum by the number of elements\n",
    "    positive_similarity_sum = 0\n",
    "    positive_similarity_cnt = 0\n",
    "\n",
    "    negative_similarity_sum = 0\n",
    "    negative_similarity_cnt = 0\n",
    "\n",
    "    for review in reviews:\n",
    "\n",
    "        # Do not consider the test document (it would be too easy)\n",
    "        if test_document != review:\n",
    "            current_tf_idf = review['tf_idf']\n",
    "\n",
    "            if(review['is_positive']):\n",
    "                positive_similarity_sum += cosine_similarity(test_tf_idf, current_tf_idf)\n",
    "                positive_similarity_cnt += 1\n",
    "\n",
    "            else:\n",
    "                negative_similarity_sum += cosine_similarity(test_tf_idf, current_tf_idf)\n",
    "                negative_similarity_cnt += 1\n",
    "\n",
    "    # Compute the mean\n",
    "    avg_positive_similarity = positive_similarity_sum / positive_similarity_cnt\n",
    "    avg_negative_similarity = negative_similarity_sum / negative_similarity_cnt\n",
    "\n",
    "    is_positive = (avg_positive_similarity > avg_negative_similarity)\n",
    "    \n",
    "    if include_similarity:\n",
    "        return is_positive, avg_positive_similarity, avg_negative_similarity\n",
    "    else:\n",
    "        return is_positive\n",
    "\n",
    "\n",
    "# Select a random review - we'll call it \"test document\"\n",
    "test_document = reviews[random.randint(0, len(reviews)-1)]\n",
    "test_tf_idf = test_document['tf_idf']\n",
    "\n",
    "identified_positive, avg_positive_similarity, avg_negative_similarity = identifyLabel(test_document, True)\n",
    "\n",
    "print(\"Positive similarity:\", avg_positive_similarity)\n",
    "print(\"Negative similarity:\", avg_negative_similarity)\n",
    "\n",
    "if(identified_positive):\n",
    "    identified_label = 'positive'\n",
    "else:\n",
    "    identified_label = 'negative'\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"The super iper advanced artificial intelligence powered by deep learning and machine learning, working on a blockchain and analyzing big data says that the review was \" + identified_label)\n",
    "\n",
    "if(test_document['is_positive']):\n",
    "    real_label = 'positive'\n",
    "else:\n",
    "    real_label = 'negative'\n",
    "    \n",
    "print(\"\\n\")\n",
    "print(\"The truth is that it was \" + real_label)\n",
    "\n",
    "print(\"\\n\")\n",
    "if(identified_label == real_label):\n",
    "    print(\"Yeah, the AI is extreamly smart!\")\n",
    "else:\n",
    "    print(\"Oh... maybe the truth is wrong?!?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To how many reviews would the correct label be assigned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 25000\t1 OK, 0 KO\n",
      "2 of 25000\t1 OK, 1 KO\n",
      "3 of 25000\t2 OK, 1 KO\n",
      "4 of 25000\t2 OK, 2 KO\n",
      "5 of 25000\t3 OK, 2 KO\n",
      "6 of 25000\t3 OK, 3 KO\n",
      "7 of 25000\t4 OK, 3 KO\n",
      "8 of 25000\t4 OK, 4 KO\n",
      "9 of 25000\t5 OK, 4 KO\n",
      "10 of 25000\t5 OK, 5 KO\n",
      "11 of 25000\t6 OK, 5 KO\n",
      "12 of 25000\t6 OK, 6 KO\n",
      "13 of 25000\t7 OK, 6 KO\n",
      "14 of 25000\t7 OK, 7 KO\n",
      "15 of 25000\t8 OK, 7 KO\n",
      "16 of 25000\t8 OK, 8 KO\n",
      "17 of 25000\t9 OK, 8 KO\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of correct and wrong label assignations on all 25000 reviews\n",
    "correct_cnt = 0\n",
    "wrong_cnt = 0\n",
    "\n",
    "i = 1\n",
    "\n",
    "for review in reviews:\n",
    "    \n",
    "    identified_is_positive = identifyLabel(review)\n",
    "    real_is_positive = review['is_positive']\n",
    "    \n",
    "    if(identified_is_positive == real_is_positive):\n",
    "        correct_cnt += 1\n",
    "    else:\n",
    "        wrong_cnt += 1\n",
    "    \n",
    "    print(str(i) + \" of \" + str(NUMBER_OF_DOCUMENTS) + \"\\t\" + str(correct_cnt) + \" OK, \" + str(wrong_cnt) + \" KO\")\n",
    "    i += 1\n",
    "    \n",
    "print(\"Correct assignations: \" + str(correct_cnt))\n",
    "print(\"Wrong assignations: \" + str(wrong_cnt))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
